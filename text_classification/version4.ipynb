{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFBertForSequenceClassification(TFBertForSequenceClassification):\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "        y_pred = self(x, training=False)\n",
    "        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def unpack_data(self, data):\n",
    "        if len(data) == 2:\n",
    "            return data[0], data[1], None\n",
    "        elif len(data) == 3:\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of elements in `data`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv6/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing CustomTFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model CustomTFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = CustomTFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts_and_labels(csv_files, categories):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_map = {category: idx for idx, category in enumerate(categories)}\n",
    "    \n",
    "    for csv_file, category in zip(csv_files, categories):\n",
    "        try:\n",
    "            with open(csv_file, 'r') as file:\n",
    "                text_data = file.read().splitlines()\n",
    "            \n",
    "            text_data = [line for line in text_data if line.strip()]\n",
    "            texts.extend(text_data)\n",
    "            labels.extend([label_map[category]] * len(text_data))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {e}\")\n",
    "    \n",
    "    return texts, labels, label_map\n",
    "\n",
    "csv_files = ['all_csv/eraser.csv', 'all_csv/keys.csv', 'all_csv/neutral_v6.csv']\n",
    "categories = ['eraser', 'keys', 'neutral']\n",
    "\n",
    "texts, labels, label_map = load_texts_and_labels(csv_files, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "labels = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "for train_index, temp_index in sss.split(input_ids_np, labels_np):\n",
    "    train_inputs, temp_inputs = input_ids_np[train_index], input_ids_np[temp_index]\n",
    "    train_labels, temp_labels = labels_np[train_index], labels_np[temp_index]\n",
    "    train_masks, temp_masks = attention_masks_np[train_index], attention_masks_np[temp_index]\n",
    "\n",
    "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "for val_index, test_index in sss_val_test.split(temp_inputs, temp_labels):\n",
    "    validation_inputs, test_inputs = temp_inputs[val_index], temp_inputs[test_index]\n",
    "    validation_labels, test_labels = temp_labels[val_index], temp_labels[test_index]\n",
    "    validation_masks, test_masks = temp_masks[val_index], temp_masks[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:35:03.993545: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 209s 1s/step - loss: 0.2766 - accuracy: 0.9324 - val_loss: 0.0849 - val_accuracy: 0.9463\n",
      "Epoch 2/4\n",
      "174/174 [==============================] - 199s 1s/step - loss: 0.0543 - accuracy: 0.9770 - val_loss: 0.0690 - val_accuracy: 0.9463\n",
      "Epoch 3/4\n",
      "174/174 [==============================] - 195s 1s/step - loss: 0.0364 - accuracy: 0.9813 - val_loss: 0.0771 - val_accuracy: 0.9530\n",
      "Epoch 4/4\n",
      "174/174 [==============================] - 204s 1s/step - loss: 0.0434 - accuracy: 0.9683 - val_loss: 0.0634 - val_accuracy: 0.9530\n"
     ]
    }
   ],
   "source": [
    "train_inputs, validation_inputs, test_inputs = map(tf.convert_to_tensor, [train_inputs, validation_inputs, test_inputs])\n",
    "train_masks, validation_masks, test_masks = map(tf.convert_to_tensor, [train_masks, validation_masks, test_masks])\n",
    "train_labels, validation_labels, test_labels = map(tf.convert_to_tensor, [train_labels, validation_labels, test_labels])\n",
    "\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "history = model.fit(\n",
    "    [train_inputs, train_masks],\n",
    "    train_labels,\n",
    "    validation_data=([validation_inputs, validation_masks], validation_labels),\n",
    "    epochs=4,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved at models/trained_v2\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"models/trained_v2\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 10s 2s/step - loss: 0.0370 - accuracy: 0.9733\n",
      "Test Accuracy: 0.9733333587646484\n",
      "5/5 [==============================] - 11s 2s/step\n",
      "Confusion Matrix:\n",
      "42 3 0\n",
      "1 44 0\n",
      "0 0 60\n",
      "\n",
      "Label mapping (index -> label name):\n",
      "0: eraser\n",
      "1: keys\n",
      "2: neutral\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate([test_inputs, test_masks], test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "predictions = model.predict([test_inputs, test_masks])\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=-1)\n",
    "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in conf_matrix:\n",
    "    print(' '.join(map(str, row)))\n",
    "\n",
    "print(\"\\nLabel mapping (index -> label name):\")\n",
    "for label_name, index in label_map.items():\n",
    "    print(f\"{index}: {label_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
