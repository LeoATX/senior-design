{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFBertForSequenceClassification(TFBertForSequenceClassification):\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        y_pred = self(x, training=False)\n",
    "        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def unpack_data(self, data):\n",
    "        if len(data) == 2:\n",
    "            return data[0], data[1], None\n",
    "        elif len(data) == 3:\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of elements in `data`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv6/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing CustomTFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model CustomTFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = CustomTFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load texts, labels, and file names\n",
    "def load_texts_labels_and_filenames(base_directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    label_map = {}\n",
    "    label_index = 0\n",
    "    \n",
    "    for label in os.listdir(base_directory):\n",
    "        label_dir = os.path.join(base_directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            if label not in label_map:\n",
    "                label_map[label] = label_index\n",
    "                label_index += 1\n",
    "            for filename in os.listdir(label_dir):\n",
    "                filepath = os.path.join(label_dir, filename)\n",
    "                if os.path.isfile(filepath) and filename.endswith('.txt'):\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        texts.append(file.read())\n",
    "                        labels.append(label_map[label])\n",
    "                        file_names.append(filename)  # Store the file name\n",
    "    \n",
    "    return texts, labels, label_map, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Base directory where your data is stored\n",
    "base_directory = 'data_v3'  # Replace with the actual path\n",
    "texts, labels, label_map, file_names = load_texts_labels_and_filenames(base_directory)\n",
    "\n",
    "# Tokenize texts\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "# Convert labels into tensors\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Use StratifiedShuffleSplit to maintain category distribution\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, temp_index in sss.split(input_ids_np, labels_np):\n",
    "    train_inputs, temp_inputs = input_ids_np[train_index], input_ids_np[temp_index]\n",
    "    train_labels, temp_labels = labels_np[train_index], labels_np[temp_index]\n",
    "    train_masks, temp_masks = attention_masks_np[train_index], attention_masks_np[temp_index]\n",
    "    train_file_names, temp_file_names = [file_names[i] for i in train_index], [file_names[i] for i in temp_index]\n",
    "\n",
    "# Now split the temp set into validation and test sets, stratifying again\n",
    "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in sss_val_test.split(temp_inputs, temp_labels):\n",
    "    validation_inputs, test_inputs = temp_inputs[val_index], temp_inputs[test_index]\n",
    "    validation_labels, test_labels = temp_labels[val_index], temp_labels[test_index]\n",
    "    validation_masks, test_masks = temp_masks[val_index], temp_masks[test_index]\n",
    "    validation_file_names, test_file_names = [temp_file_names[i] for i in val_index], [temp_file_names[i] for i in test_index]\n",
    "\n",
    "# Convert everything back to tensors if needed for the model input\n",
    "train_inputs = tf.convert_to_tensor(train_inputs)\n",
    "validation_inputs = tf.convert_to_tensor(validation_inputs)\n",
    "test_inputs = tf.convert_to_tensor(test_inputs)\n",
    "\n",
    "train_masks = tf.convert_to_tensor(train_masks)\n",
    "validation_masks = tf.convert_to_tensor(validation_masks)\n",
    "test_masks = tf.convert_to_tensor(test_masks)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "validation_labels = tf.convert_to_tensor(validation_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 128)\n",
      "(105, 128)\n",
      "(23, 128)\n"
     ]
    }
   ],
   "source": [
    "print(validation_masks.shape)\n",
    "print(train_masks.shape)\n",
    "print(test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer, loss function, and metrics\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "27/27 [==============================] - 36s 1s/step - loss: 0.3785 - accuracy: 0.8857 - val_loss: 0.1587 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "27/27 [==============================] - 38s 1s/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "27/27 [==============================] - 32s 1s/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "27/27 [==============================] - 31s 1s/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_inputs, train_masks],\n",
    "    train_labels,\n",
    "    validation_data=([validation_inputs, validation_masks], validation_labels),\n",
    "    epochs=4,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_inputs, test_masks], test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x41087fce0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x41087fce0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted labels on the test set: [0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test set (optional)\n",
    "predictions = model.predict([test_inputs, test_masks])\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=-1)\n",
    "print(\"Predicted labels on the test set:\", predicted_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "15 0\n",
      "0 8\n",
      "\n",
      "Label mapping (index -> label name):\n",
      "0: neutral\n",
      "1: commands\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_np = predicted_labels.numpy()\n",
    "true_labels_np = test_labels\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in conf_matrix:\n",
    "    print(' '.join(map(str, row)))\n",
    "\n",
    "# Optionally print the labels for reference\n",
    "print(\"\\nLabel mapping (index -> label name):\")\n",
    "for label_name, index in label_map.items():\n",
    "    print(f\"{index}: {label_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "File names classified as commands:\n",
      "keys_3.txt\n",
      "eraser_4.txt\n",
      "keys_6.txt\n",
      "keys_12.txt\n",
      "eraser_10.txt\n",
      "keys_4.txt\n",
      "eraser_14.txt\n",
      "eraser_21.txt\n"
     ]
    }
   ],
   "source": [
    "commands_label = label_map['commands']  # Assuming \"commands\" is one of the labels\n",
    "\n",
    "predictions = model.predict([test_inputs, test_masks])\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=-1)\n",
    "\n",
    "predicted_labels_np = predicted_labels.numpy()\n",
    "\n",
    "# Define the label index for commands\n",
    "commands_label = label_map['commands']\n",
    "\n",
    "commands_indices = np.where(predicted_labels_np == commands_label)[0]\n",
    "\n",
    "commands_filenames = [test_file_names[i] for i in commands_indices]\n",
    "\n",
    "print(\"File names classified as commands:\")\n",
    "for fname in commands_filenames:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at models/trained_v1 were not used when initializing CustomTFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing CustomTFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomTFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of CustomTFBertForSequenceClassification were initialized from the model checkpoint at models/trained_v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use CustomTFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted labels from the new model for command inputs: [2 0 2 2 0 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Optionally, use the new model for further processing\n",
    "if len(commands_indices) > 0:\n",
    "    commands_inputs = tf.gather(test_inputs, commands_indices)\n",
    "    commands_masks = tf.gather(test_masks, commands_indices)\n",
    "\n",
    "    # Load the new model for further processing\n",
    "    new_model = CustomTFBertForSequenceClassification.from_pretrained('models/trained_v1')  # Replace with your new model path\n",
    "    new_tokenizer = BertTokenizer.from_pretrained('models/trained_v1')  # Load new tokenizer if different\n",
    "\n",
    "    # Predict using the new model on the command inputs\n",
    "    new_predictions = new_model.predict([commands_inputs, commands_masks])\n",
    "    new_predicted_labels = tf.argmax(new_predictions.logits, axis=-1)\n",
    "    \n",
    "    print(\"Predicted labels from the new model for command inputs:\", new_predicted_labels.numpy())\n",
    "else:\n",
    "    print(\"No inputs classified as commands.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
