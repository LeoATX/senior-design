{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate file: data_v1/neutral/neutral_171.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_164.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_35.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_203.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_22.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_163.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_215.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_200.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_176.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_23.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_47.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_53.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_113.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_107.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_42.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_81.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_94.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_82.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_101.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_115.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_54.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_66.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_72.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_245.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_250.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_98.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_132.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_7.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_67.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_88.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_136.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_2.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_76.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_48.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_60.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_135.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_121.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_242.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_75.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_190.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_146.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_187.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_11.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_193.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_39.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_150.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_232.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_226.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_227.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_233.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_179.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_38.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_10.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_182.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_169.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_141.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_236.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_197.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_29.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_17.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_142.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_208.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_220.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_235.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_221.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_209.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_157.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_194.txt\n",
      "Removing duplicate file: data_v1/neutral/neutral_16.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "def hash_file(file_path):\n",
    "    \"\"\"Generate a hash for the content of a file.\"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def deduplicate_txt_files(directory):\n",
    "    # Dictionary to store the hash of each file and the corresponding file path\n",
    "    file_hashes = {}\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Generate a hash for the file's content\n",
    "            file_hash = hash_file(file_path)\n",
    "\n",
    "            if file_hash in file_hashes:\n",
    "                # If the hash is already seen, it's a duplicate, so remove it\n",
    "                print(f\"Removing duplicate file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "            else:\n",
    "                # Otherwise, store the hash and file path\n",
    "                file_hashes[file_hash] = file_path\n",
    "\n",
    "# Example usage\n",
    "txt_files_directory = 'data_v1/neutral'\n",
    "deduplicate_txt_files(txt_files_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
