{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "class CustomTFBertForSequenceClassification(TFBertForSequenceClassification):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data manually\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data manually (same as train_step)\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = self(x, training=False)\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def unpack_data(self, data):\n",
    "        if len(data) == 2:\n",
    "            return data[0], data[1], None  # inputs, labels, sample_weights\n",
    "        elif len(data) == 3:\n",
    "            return data  # inputs, labels, sample_weights\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of elements in `data`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing CustomTFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model CustomTFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the custom BERT model for sequence classification\n",
    "model = CustomTFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'eraser': 0, 'neutral': 1, 'keys': 2}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to load texts and labels from subdirectories\n",
    "def load_texts_and_labels_from_directories(base_directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_map = {}  # Dictionary to map string labels to integers\n",
    "    \n",
    "    label_index = 0\n",
    "    \n",
    "    # Loop through each subdirectory (each subdirectory name is a label)\n",
    "    for label in os.listdir(base_directory):\n",
    "        label_dir = os.path.join(base_directory, label)\n",
    "        \n",
    "        if os.path.isdir(label_dir):  # Ensure it's a directory\n",
    "            # Map the label to an integer if it's not already in the map\n",
    "            if label not in label_map:\n",
    "                label_map[label] = label_index\n",
    "                label_index += 1\n",
    "\n",
    "            for filename in os.listdir(label_dir):\n",
    "                filepath = os.path.join(label_dir, filename)\n",
    "                \n",
    "                if os.path.isfile(filepath) and filename.endswith('.txt'):\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        texts.append(file.read())  # Read the content of the file\n",
    "                        labels.append(label_map[label])  # Use the mapped integer label\n",
    "    \n",
    "    return texts, labels, label_map\n",
    "\n",
    "# Function to tokenize the input texts\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Sample usage\n",
    "base_directory = 'data_v1'  # Replace with the actual path to your text files (each subdirectory is a label)\n",
    "texts, labels, label_map = load_texts_and_labels_from_directories(base_directory)\n",
    "\n",
    "# Assuming you already have your tokenizer initialized\n",
    "# tokenizer = ... (initialize your tokenizer here)\n",
    "\n",
    "# Tokenize the texts loaded from files\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "# Convert labels into tensors\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "# Optionally, print the label map to see the mapping from string labels to integers\n",
    "print(\"Label mapping:\", label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Convert texts and labels into tensors\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "labels = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets (you should use real data for training)\n",
    "train_size = int(0.8 * len(texts))\n",
    "train_inputs, validation_inputs = input_ids[:train_size], input_ids[train_size:]\n",
    "train_labels, validation_labels = labels[:train_size], labels[train_size:]\n",
    "train_masks, validation_masks = attention_masks[:train_size], attention_masks[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer, loss function, and metrics\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "52/52 [==============================] - 26s 417ms/step - loss: 0.8892 - accuracy: 0.4615 - val_loss: 0.8129 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "52/52 [==============================] - 23s 449ms/step - loss: 0.3238 - accuracy: 0.9808 - val_loss: 0.3778 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "52/52 [==============================] - 20s 385ms/step - loss: 0.1241 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "52/52 [==============================] - 20s 376ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_inputs, train_masks], \n",
    "    train_labels,\n",
    "    validation_data=([validation_inputs, validation_masks], validation_labels),\n",
    "    epochs=4,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate([validation_inputs, validation_masks], validation_labels)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
