{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv6/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFBertForSequenceClassification(TFBertForSequenceClassification):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data manually\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data manually\n",
    "        x, y, sample_weight = self.unpack_data(data)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = self(x, training=False)\n",
    "        # Compute loss\n",
    "        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n",
    "\n",
    "        # Update the metrics\n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def unpack_data(self, data):\n",
    "        if len(data) == 2:\n",
    "            return data[0], data[1], None  # inputs, labels, sample_weights\n",
    "        elif len(data) == 3:\n",
    "            return data  # inputs, labels, sample_weights\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of elements in `data`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv6/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing CustomTFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model CustomTFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = CustomTFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load texts and labels from subdirectories\n",
    "def load_texts_and_labels_from_directories(base_directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "    label_index = 0\n",
    "    \n",
    "    for label in os.listdir(base_directory):\n",
    "        label_dir = os.path.join(base_directory, label)\n",
    "        if os.path.isdir(label_dir):\n",
    "            if label not in label_map:\n",
    "                label_map[label] = label_index\n",
    "                label_index += 1\n",
    "            for filename in os.listdir(label_dir):\n",
    "                filepath = os.path.join(label_dir, filename)\n",
    "                if os.path.isfile(filepath) and filename.endswith('.txt'):\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        texts.append(file.read())\n",
    "                        labels.append(label_map[label])\n",
    "    return texts, labels, label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Base directory where your data is stored\n",
    "base_directory = 'data_v1'  # Replace with the actual path\n",
    "texts, labels, label_map = load_texts_and_labels_from_directories(base_directory)\n",
    "\n",
    "# Tokenize texts\n",
    "input_ids, attention_masks = tokenize_texts(texts, tokenizer)\n",
    "\n",
    "# Convert labels into tensors\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "# Convert TensorFlow tensors to NumPy arrays for splitting\n",
    "input_ids_np = input_ids.numpy()\n",
    "attention_masks_np = attention_masks.numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Use StratifiedShuffleSplit to maintain category distribution\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, temp_index in sss.split(input_ids_np, labels_np):\n",
    "    train_inputs, temp_inputs = input_ids_np[train_index], input_ids_np[temp_index]\n",
    "    train_labels, temp_labels = labels_np[train_index], labels_np[temp_index]\n",
    "    train_masks, temp_masks = attention_masks_np[train_index], attention_masks_np[temp_index]\n",
    "\n",
    "# Now split the temp set into validation and test sets, stratifying again\n",
    "sss_val_test = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "\n",
    "for val_index, test_index in sss_val_test.split(temp_inputs, temp_labels):\n",
    "    validation_inputs, test_inputs = temp_inputs[val_index], temp_inputs[test_index]\n",
    "    validation_labels, test_labels = temp_labels[val_index], temp_labels[test_index]\n",
    "    validation_masks, test_masks = temp_masks[val_index], temp_masks[test_index]\n",
    "\n",
    "# Convert everything back to tensors if needed for the model input\n",
    "train_inputs = tf.convert_to_tensor(train_inputs)\n",
    "validation_inputs = tf.convert_to_tensor(validation_inputs)\n",
    "test_inputs = tf.convert_to_tensor(test_inputs)\n",
    "\n",
    "train_masks = tf.convert_to_tensor(train_masks)\n",
    "validation_masks = tf.convert_to_tensor(validation_masks)\n",
    "test_masks = tf.convert_to_tensor(test_masks)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "validation_labels = tf.convert_to_tensor(validation_labels)\n",
    "test_labels = tf.convert_to_tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 128)\n",
      "(105, 128)\n",
      "(23, 128)\n"
     ]
    }
   ],
   "source": [
    "print(validation_masks.shape)\n",
    "print(train_masks.shape)\n",
    "print(test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer, loss function, and metrics\n",
    "optimizer = Adam(learning_rate=2e-5, epsilon=1e-8)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 12:57:46.882144: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 40s 1s/step - loss: 0.7971 - accuracy: 0.6381 - val_loss: 0.4295 - val_accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "27/27 [==============================] - 31s 1s/step - loss: 0.2735 - accuracy: 0.9714 - val_loss: 0.1127 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "27/27 [==============================] - 29s 1s/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "27/27 [==============================] - 30s 1s/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0258 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_inputs, train_masks],\n",
    "    train_labels,\n",
    "    validation_data=([validation_inputs, validation_masks], validation_labels),\n",
    "    epochs=4,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate([test_inputs, test_masks], test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted labels on the test set: [0 2 2 1 1 2 1 1 1 1 1 1 1 2 0 1 1 1 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the test set (optional)\n",
    "predictions = model.predict([test_inputs, test_masks])\n",
    "predicted_labels = tf.argmax(predictions.logits, axis=-1)\n",
    "print(\"Predicted labels on the test set:\", predicted_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "4 0 0\n",
      "0 15 0\n",
      "0 0 4\n",
      "\n",
      "Label mapping (index -> label name):\n",
      "0: eraser\n",
      "1: neutral\n",
      "2: keys\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_np = predicted_labels.numpy()\n",
    "true_labels_np = test_labels\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "for row in conf_matrix:\n",
    "    print(' '.join(map(str, row)))\n",
    "\n",
    "# Optionally print the labels for reference\n",
    "print(\"\\nLabel mapping (index -> label name):\")\n",
    "for label_name, index in label_map.items():\n",
    "    print(f\"{index}: {label_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
