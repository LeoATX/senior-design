{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mBertTextClassifier(\n\u001b[1;32m      5\u001b[0m     backbone, num_classes, preprocessor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "keras_nlp.models.BertTextClassifier(\n",
    "    backbone, num_classes, preprocessor=None, activation=None, dropout=0.1, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/opt/anaconda3/envs/myenv1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize input sentences\n",
    "def encode_sentences(sentences, tokenizer, max_length=128):\n",
    "    # Ensure no empty or invalid sentences are present\n",
    "    if not all(sentences):\n",
    "        raise ValueError(\"Some sentences are empty or invalid.\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        sentences,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf',\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    \n",
    "    # Ensure that input_ids and attention_mask are returned properly\n",
    "    input_ids = inputs.get('input_ids')\n",
    "    attention_mask = inputs.get('attention_mask')\n",
    "\n",
    "    if input_ids is None or attention_mask is None:\n",
    "        raise ValueError(\"Tokenization returned None for input_ids or attention_mask.\")\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\"Hello, how are you?\", \"I am fine, thanks.\"]\n",
    "input_ids, attention_mask = encode_sentences(sentences, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: (2, 128)\n",
      "Attention Mask shape: (2, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the model architecture\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask_layer = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# BERT model\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_mask_layer)\n",
    "pooled_output = bert_output.pooler_output\n",
    "\n",
    "# Add a classification layer\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(pooled_output)\n",
    "\n",
    "# Create the Keras model\n",
    "model = tf.keras.Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the input shapes for debugging\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Mask shape: {attention_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.8081 - accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 1s 752ms/step - loss: 0.6488 - accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 0.4834 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x38e4a5dc0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy labels (1 for positive, 0 for negative) for binary classification\n",
    "labels = [1, 0]\n",
    "labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "# Train the model\n",
    "model.fit([input_ids, attention_mask], labels, epochs=3, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.34069943]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\"This is a test sentence.\"]\n",
    "test_input_ids, test_attention_mask = encode_sentences(test_sentences, tokenizer)\n",
    "\n",
    "predictions = model.predict([test_input_ids, test_attention_mask])\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
